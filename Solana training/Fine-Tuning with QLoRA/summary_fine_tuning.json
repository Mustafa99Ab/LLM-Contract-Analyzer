{
  "experiment": "Fine-Tuning with QLoRA",
  "experiment_id": 2,
  "model": {
    "name": "Llama-3.1-8B-Instruct",
    "quantization": "4-bit NF4",
    "parameters": "8B"
  },
  "method": {
    "type": "Fine-Tuning",
    "technique": "QLoRA + SFTTrainer",
    "description": "Low-Rank Adaptation with completion-only training",
    "training_required": true
  },
  "lora_config": {
    "r": 64,
    "alpha": 16,
    "dropout": 0.1,
    "target_modules": [
      "up_proj",
      "k_proj",
      "gate_proj",
      "o_proj",
      "q_proj",
      "down_proj",
      "v_proj"
    ]
  },
  "training_config": {
    "epochs": 3,
    "learning_rate": 0.0002,
    "batch_size": 2,
    "gradient_accumulation": 4,
    "effective_batch_size": 8,
    "max_seq_length": 1024,
    "warmup_ratio": 0.03
  },
  "dataset": {
    "total": 140,
    "train": 112,
    "val": 14,
    "test": 14,
    "vulnerability_types": 7
  },
  "results": {
    "overall_accuracy": 0.8571,
    "per_vulnerability": {
      "Bump Seed": {
        "Accuracy": 1.0,
        "Precision": 1.0,
        "Recall": 1.0,
        "F1-score": 1.0
      },
      "CPI": {
        "Accuracy": 0.5,
        "Precision": 0.5,
        "Recall": 1.0,
        "F1-score": 0.67
      },
      "DoS": {
        "Accuracy": 1.0,
        "Precision": 1.0,
        "Recall": 1.0,
        "F1-score": 1.0
      },
      "Integer Flow": {
        "Accuracy": 0.5,
        "Precision": 0.0,
        "Recall": 0.0,
        "F1-score": 0.0
      },
      "Missing Key Check": {
        "Accuracy": 1.0,
        "Precision": 1.0,
        "Recall": 1.0,
        "F1-score": 1.0
      },
      "Type Confusion": {
        "Accuracy": 1.0,
        "Precision": 1.0,
        "Recall": 1.0,
        "F1-score": 1.0
      },
      "Unchecked Calls": {
        "Accuracy": 1.0,
        "Precision": 1.0,
        "Recall": 1.0,
        "F1-score": 1.0
      }
    },
    "average": {
      "Accuracy": 0.86,
      "Precision": 0.79,
      "Recall": 0.86,
      "F1-score": 0.81
    }
  },
  "confusion_matrix": {
    "TP": 6,
    "FN": 1,
    "FP": 1,
    "TN": 6
  },
  "references": [
    "Dettmers et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314"
  ]
}