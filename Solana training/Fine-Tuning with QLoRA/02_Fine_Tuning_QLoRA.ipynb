{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Experiment 2: Fine-Tuning with QLoRA\n\n**Objective:** Fine-tune LLaMA-3.1-8B-Instruct on Solana smart contract vulnerability detection.\n\n**Method:** QLoRA (Quantized Low-Rank Adaptation) with SFTTrainer for efficient fine-tuning.\n\n**Key Innovation:** Using DataCollatorForCompletionOnlyLM to train only on classification output.\n\n**References:**\n- Dettmers et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314\n- Boi & Esposito (2025). Prompt Engineering vs. Fine-Tuning for LLM-Based Vulnerability Detection.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Environment Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport os\nimport warnings\n\n# Suppress all warnings for clean output\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n\nprint(\"=\" * 50)\nprint(\"ENVIRONMENT CHECK\")\nprint(\"=\" * 50)\n\nif torch.cuda.is_available():\n    GPU_NAME = torch.cuda.get_device_name(0)\n    GPU_MEMORY = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {GPU_NAME}\")\n    print(f\"Memory: {GPU_MEMORY:.1f} GB\")\n    print(\"Status: Ready\")\nelse:\n    print(\"ERROR: GPU not detected!\")\n    print(\"Go to: Settings -> Accelerator -> GPU T4 x2\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%capture\n# Install packages silently\n!pip install -q bitsandbytes accelerate\n!pip install -q peft==0.9.0\n!pip install -q trl==0.12.0",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Packages installed successfully.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Imports & Authentication",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport logging\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom tqdm import tqdm\nfrom datasets import Dataset\nfrom huggingface_hub import login\n\n# Suppress all logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"accelerate\").setLevel(logging.ERROR)\nlogging.getLogger(\"peft\").setLevel(logging.ERROR)\nlogging.getLogger(\"trl\").setLevel(logging.ERROR)\n\n# Authentication\nfrom kaggle_secrets import UserSecretsClient\nsecrets = UserSecretsClient()\nHF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\nlogin(token=HF_TOKEN, add_to_git_credential=False)\n\nprint(\"Authentication successful.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Load Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find dataset path\nPOSSIBLE_PATHS = [\n    \"/kaggle/input/solana-dataset/solana_140s_final.json\",\n    \"/kaggle/input/solana_140s_final.json\",\n    \"/kaggle/working/solana_140s_final.json\"\n]\n\nDATASET_PATH = None\nfor path in POSSIBLE_PATHS:\n    if os.path.exists(path):\n        DATASET_PATH = path\n        break\n\nif DATASET_PATH is None:\n    raise FileNotFoundError(\"Dataset not found. Please upload solana_140s_final.json\")\n\nwith open(DATASET_PATH, 'r') as f:\n    dataset = json.load(f)\n\nprint(f\"Dataset: {len(dataset)} samples\")\nprint(f\"Path: {DATASET_PATH}\")\nprint(\"\\nVulnerability Types:\")\nfor vtype, count in sorted(Counter(s['vulnerability_type'] for s in dataset).items()):\n    print(f\"  {vtype}: {count}\")\nprint(f\"\\nLabels: VULNERABLE={sum(1 for s in dataset if s['label']=='VULNERABLE')}, SAFE={sum(1 for s in dataset if s['label']=='SAFE')}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Data Split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Stratified split by vulnerability type (80% train, 10% val, 10% test)\nby_vuln_type = defaultdict(list)\nfor sample in dataset:\n    by_vuln_type[sample['vulnerability_type']].append(sample)\n\ntrain_data, val_data, test_data = [], [], []\n\nfor vtype, samples in by_vuln_type.items():\n    labels = [s['label'] for s in samples]\n    train_samples, temp_samples = train_test_split(samples, test_size=0.2, stratify=labels, random_state=42)\n    temp_labels = [s['label'] for s in temp_samples]\n    val_samples, test_samples = train_test_split(temp_samples, test_size=0.5, stratify=temp_labels, random_state=42)\n    train_data.extend(train_samples)\n    val_data.extend(val_samples)\n    test_data.extend(test_samples)\n\nprint(\"Data Split:\")\nprint(f\"  Train: {len(train_data)} (80%)\")\nprint(f\"  Val:   {len(val_data)} (10%)\")\nprint(f\"  Test:  {len(test_data)} (10%)\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create HuggingFace datasets\ntrain_dataset = Dataset.from_list([{\"text\": s[\"text\"]} for s in train_data])\nval_dataset = Dataset.from_list([{\"text\": s[\"text\"]} for s in val_data])\n\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Val dataset: {len(val_dataset)} samples\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nMODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# 4-bit quantization (QLoRA)\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=quant_config,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n    low_cpu_mem_usage=True,\n    use_cache=False  # Disable cache for training\n)\n\nprint(f\"\\nModel: {MODEL_ID}\")\nprint(f\"Parameters: {model.num_parameters():,}\")\nprint(f\"Quantization: 4-bit NF4\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. LoRA Configuration\n\nLoRA (Low-Rank Adaptation) adds small trainable matrices to the model while keeping the original weights frozen.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from peft import LoraConfig, prepare_model_for_kbit_training\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA configuration\nLORA_CONFIG = LoraConfig(\n    r=64,                    # Rank\n    lora_alpha=16,           # Scaling factor  \n    lora_dropout=0.1,        # Dropout\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nprint(\"=\" * 50)\nprint(\"LoRA Configuration\")\nprint(\"=\" * 50)\nprint(f\"  Rank (r): {LORA_CONFIG.r}\")\nprint(f\"  Alpha: {LORA_CONFIG.lora_alpha}\")\nprint(f\"  Dropout: {LORA_CONFIG.lora_dropout}\")\nprint(f\"  Target: Attention + MLP layers\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Training Configuration\n\n### Why SFTTrainer with DataCollatorForCompletionOnlyLM?\n\nStandard training computes loss on the entire text (prompt + response), which can cause:\n- Pattern memorization instead of learning\n- Mode collapse (always predicting one class)\n\nWith `DataCollatorForCompletionOnlyLM`, the loss is computed **only on the response** (VULNERABLE/SAFE), making the model learn to classify rather than memorize.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n\n# Response template - loss computed only after this marker\nRESPONSE_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n# Data collator for completion-only training\ndata_collator = DataCollatorForCompletionOnlyLM(\n    response_template=RESPONSE_TEMPLATE,\n    tokenizer=tokenizer\n)\n\nprint(\"Data Collator: DataCollatorForCompletionOnlyLM\")\nprint(f\"Response Template: {RESPONSE_TEMPLATE}\")\nprint(\"\\nTraining will focus ONLY on classification output.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training arguments\nTRAINING_CONFIG = SFTConfig(\n    output_dir=\"/kaggle/working/checkpoints\",\n    \n    # Training parameters\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    \n    # Optimizer\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    \n    # Precision\n    fp16=True,\n    \n    # Logging\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    \n    # SFT specific\n    max_seq_length=1024,\n    packing=False,\n    \n    # Other\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n\nprint(\"=\" * 50)\nprint(\"Training Configuration\")\nprint(\"=\" * 50)\nprint(f\"  Epochs: {TRAINING_CONFIG.num_train_epochs}\")\nprint(f\"  Batch size: {TRAINING_CONFIG.per_device_train_batch_size}\")\nprint(f\"  Gradient accumulation: {TRAINING_CONFIG.gradient_accumulation_steps}\")\nprint(f\"  Effective batch size: {TRAINING_CONFIG.per_device_train_batch_size * TRAINING_CONFIG.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {TRAINING_CONFIG.learning_rate}\")\nprint(f\"  Max sequence length: {TRAINING_CONFIG.max_seq_length}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=TRAINING_CONFIG,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=LORA_CONFIG,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"SFTTrainer initialized.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 50)\nprint(\"STARTING TRAINING\")\nprint(\"Estimated time: 20-30 minutes\")\nprint(\"=\" * 50)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 50)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save model\nMODEL_OUTPUT = \"/kaggle/working/solana-vuln-model\"\ntrainer.save_model(MODEL_OUTPUT)\ntokenizer.save_pretrained(MODEL_OUTPUT)\nprint(f\"Model saved to: {MODEL_OUTPUT}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def extract_code(sample):\n    \"\"\"Extract code content from formatted sample.\"\"\"\n    text = sample['text']\n    start_marker = '<|start_header_id|>user<|end_header_id|>'\n    end_marker = '<|eot_id|><|start_header_id|>assistant'\n    \n    start_idx = text.find(start_marker)\n    end_idx = text.find(end_marker)\n    \n    if start_idx != -1 and end_idx != -1:\n        return text[start_idx + len(start_marker):end_idx].strip()\n    return text[:1000]\n\ndef predict_fine_tuned(sample):\n    \"\"\"Prediction using fine-tuned model.\"\"\"\n    code = extract_code(sample)\n    \n    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a smart contract security analyzer.\nYou analyze Solana smart contracts written in Rust and identify vulnerabilities.\nClassify the code as either VULNERABLE or SAFE.\nRespond with only one word: VULNERABLE or SAFE.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{code}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=10,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip().upper()\n    first_word = response.split()[0] if response.split() else \"\"\n    \n    return 'VULNERABLE' if 'VULN' in first_word else 'SAFE'\n\nprint(\"Prediction function ready.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 50)\nprint(\"RUNNING EVALUATION\")\nprint(\"=\" * 50)\n\nresults = []\nfor sample in tqdm(test_data, desc=\"Evaluating\"):\n    pred = predict_fine_tuned(sample)\n    results.append({\n        'vulnerability_type': sample['vulnerability_type'],\n        'ground_truth': sample['label'],\n        'prediction': pred,\n        'correct': sample['label'] == pred\n    })\n\nprint(\"\\nEvaluation complete.\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate metrics per vulnerability type\nmetrics_by_type = {}\nvuln_types = sorted(set(r['vulnerability_type'] for r in results))\n\nfor vtype in vuln_types:\n    type_results = [r for r in results if r['vulnerability_type'] == vtype]\n    gt = [r['ground_truth'] for r in type_results]\n    pred = [r['prediction'] for r in type_results]\n    \n    metrics_by_type[vtype] = {\n        'Accuracy': round(accuracy_score(gt, pred), 2),\n        'Precision': round(precision_score(gt, pred, pos_label='VULNERABLE', zero_division=0), 2),\n        'Recall': round(recall_score(gt, pred, pos_label='VULNERABLE', zero_division=0), 2),\n        'F1-score': round(f1_score(gt, pred, pos_label='VULNERABLE', zero_division=0), 2)\n    }\n\n# Calculate averages\navg_metrics = {\n    'Accuracy': round(sum(m['Accuracy'] for m in metrics_by_type.values()) / len(metrics_by_type), 2),\n    'Precision': round(sum(m['Precision'] for m in metrics_by_type.values()) / len(metrics_by_type), 2),\n    'Recall': round(sum(m['Recall'] for m in metrics_by_type.values()) / len(metrics_by_type), 2),\n    'F1-score': round(sum(m['F1-score'] for m in metrics_by_type.values()) / len(metrics_by_type), 2)\n}\n\n# Display\nprint(\"=\" * 70)\nprint(\"RESULTS: Experiment 2 - Fine-Tuning with QLoRA\")\nprint(\"=\" * 70)\nprint(f\"{'Vulnerability':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-score':<12}\")\nprint(\"-\" * 70)\nfor vtype in vuln_types:\n    m = metrics_by_type[vtype]\n    print(f\"{vtype:<20} {m['Accuracy']:<12} {m['Precision']:<12} {m['Recall']:<12} {m['F1-score']:<12}\")\nprint(\"-\" * 70)\nprint(f\"{'Average':<20} {avg_metrics['Accuracy']:<12} {avg_metrics['Precision']:<12} {avg_metrics['Recall']:<12} {avg_metrics['F1-score']:<12}\")\nprint(\"=\" * 70)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Confusion Matrix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "all_gt = [r['ground_truth'] for r in results]\nall_pred = [r['prediction'] for r in results]\ncm = confusion_matrix(all_gt, all_pred, labels=['VULNERABLE', 'SAFE'])\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n            xticklabels=['VULNERABLE', 'SAFE'],\n            yticklabels=['VULNERABLE', 'SAFE'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Experiment 2: Fine-Tuning (QLoRA) - Confusion Matrix')\nplt.tight_layout()\nplt.savefig('/kaggle/working/cm_fine_tuning.png', dpi=150)\nplt.show()\n\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  TP (detected vulnerabilities): {cm[0,0]}\")\nprint(f\"  FN (missed vulnerabilities):   {cm[0,1]}\")\nprint(f\"  FP (false alarms):             {cm[1,0]}\")\nprint(f\"  TN (correct safe):             {cm[1,1]}\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Save Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('/kaggle/working/results_fine_tuning.csv', index=False)\n\n# Save summary JSON\nsummary = {\n    'experiment': 'Fine-Tuning with QLoRA',\n    'experiment_id': 2,\n    'model': {\n        'name': 'Llama-3.1-8B-Instruct',\n        'quantization': '4-bit NF4',\n        'parameters': '8B'\n    },\n    'method': {\n        'type': 'Fine-Tuning',\n        'technique': 'QLoRA + SFTTrainer',\n        'description': 'Low-Rank Adaptation with completion-only training',\n        'training_required': True\n    },\n    'lora_config': {\n        'r': LORA_CONFIG.r,\n        'alpha': LORA_CONFIG.lora_alpha,\n        'dropout': LORA_CONFIG.lora_dropout,\n        'target_modules': list(LORA_CONFIG.target_modules)\n    },\n    'training_config': {\n        'epochs': TRAINING_CONFIG.num_train_epochs,\n        'learning_rate': TRAINING_CONFIG.learning_rate,\n        'batch_size': TRAINING_CONFIG.per_device_train_batch_size,\n        'gradient_accumulation': TRAINING_CONFIG.gradient_accumulation_steps,\n        'effective_batch_size': TRAINING_CONFIG.per_device_train_batch_size * TRAINING_CONFIG.gradient_accumulation_steps,\n        'max_seq_length': TRAINING_CONFIG.max_seq_length,\n        'warmup_ratio': TRAINING_CONFIG.warmup_ratio\n    },\n    'dataset': {\n        'total': len(dataset),\n        'train': len(train_data),\n        'val': len(val_data),\n        'test': len(test_data),\n        'vulnerability_types': 7\n    },\n    'results': {\n        'overall_accuracy': round(accuracy_score(all_gt, all_pred), 4),\n        'per_vulnerability': metrics_by_type,\n        'average': avg_metrics\n    },\n    'confusion_matrix': {\n        'TP': int(cm[0,0]),\n        'FN': int(cm[0,1]),\n        'FP': int(cm[1,0]),\n        'TN': int(cm[1,1])\n    },\n    'references': [\n        'Dettmers et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314'\n    ]\n}\n\nwith open('/kaggle/working/summary_fine_tuning.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"Files saved:\")\nprint(\"  - results_fine_tuning.csv\")\nprint(\"  - summary_fine_tuning.json\")\nprint(\"  - cm_fine_tuning.png\")\nprint(\"  - solana-vuln-model/ (adapter files)\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\" * 50)\nprint(\"EXPERIMENT 2 COMPLETE\")\nprint(\"=\" * 50)\nprint(f\"\\nOverall Accuracy: {summary['results']['overall_accuracy']:.2%}\")\nprint(f\"Average F1-Score: {avg_metrics['F1-score']}\")\nprint(f\"\\nMethod: QLoRA + SFTTrainer\")\nprint(f\"Model: LLaMA-3.1-8B-Instruct (fine-tuned)\")\nprint(f\"\\nIMPORTANT: Download solana-vuln-model/ for Experiment 3\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
